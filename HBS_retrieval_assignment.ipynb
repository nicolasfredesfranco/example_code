{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b781e47-2986-4c1d-b1f6-28361312aa93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#---CODE MODIFICATION: NEW LIBRERIES TO BE IMPORTED\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2384d13-941a-4c49-b903-68c5cf55ac15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDS repo already cloned. Skipping git clone step.\n"
     ]
    }
   ],
   "source": [
    "#clone the git repo that contains the data and additional information about the dataset\n",
    "#---if the repo is already cloned, skip this step\n",
    "if not os.path.exists('WANDS'):\n",
    "    # Only clone if directory does not exist\n",
    "    !git clone https://github.com/wayfair/WANDS.git\n",
    "else:\n",
    "    print(\"WANDS repo already cloned. Skipping git clone step.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f735c1c",
   "metadata": {},
   "source": [
    "# TF-IDF and information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ce17e",
   "metadata": {},
   "source": [
    "### üîç Information-Theoretic View of TF‚ÄìIDF (Motivation)\n",
    "\n",
    "In this section we briefly analyze the original TF‚ÄìIDF technique from an information-theoretic perspective, as motivation for the methods we will use in this assignment.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Vector Representation of Product Documents and Queries\n",
    "\n",
    "Let $V$ be the vocabulary of all unique terms (words) in the corpus, with size $|V|$.\n",
    "\n",
    "- Each **product document** $d$ is represented as a vector\n",
    "  $$\n",
    "  \\mathbf{v}^{(d)} = (w_{t_1,d}, w_{t_2,d}, \\dots, w_{t_{|V|},d}) \\in \\mathbb{R}^{|V|}\n",
    "  $$\n",
    "- Each **query** $q$ is represented in the same space:\n",
    "  $$\n",
    "  \\mathbf{v}^{(q)} = (w_{t_1,q}, w_{t_2,q}, \\dots, w_{t_{|V|},q}) \\in \\mathbb{R}^{|V|}\n",
    "  $$\n",
    "\n",
    "Here, $w_{t,d}$ is the TF‚ÄìIDF weight of term $t$ in product document $d$.  \n",
    "The goal is to embed both product documents and queries in a common vector space so that we can compute similarity (e.g., cosine similarity) to rank products.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Term Frequency (TF) as Local Probability\n",
    "\n",
    "Let $t$ be a term and $d$ a product document. A normalized term frequency is:\n",
    "\n",
    "$$\n",
    "\\mathrm{TF}(t,d) = \\frac{f_{t,d}}{\\sum_j f_{j,d}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $f_{t,d}$ is the number of occurrences of term $t$ in product document $d$,\n",
    "- $\\sum_j f_{j,d}$ is the total number of term occurrences in $d$.\n",
    "\n",
    "From a probabilistic viewpoint, this can be interpreted as an **empirical estimate of the local probability** of seeing $t$ in product document $d$:\n",
    "\n",
    "$$\n",
    "\\mathrm{TF}(t,d) \\approx \\hat{p}(t \\mid d)\n",
    "$$\n",
    "\n",
    "So TF captures how important $t$ is **within this particular product document** $d$.  \n",
    "It is inherently **product-document‚Äìdependent**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Inverse Document Frequency (IDF) as Global Surprise\n",
    "\n",
    "Now consider the corpus as a whole:\n",
    "\n",
    "- $N$ = total number of product documents.\n",
    "- $df(t)$ = number of product documents that contain term $t$.\n",
    "\n",
    "The inverse document frequency is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{IDF}(t) = \\log\\left(\\frac{N}{1 + df(t)}\\right)\n",
    "$$\n",
    "\n",
    "If we approximate the **global probability** that a random product document contains $t$ by\n",
    "\n",
    "$$\n",
    "\\hat{p}(t) \\approx \\frac{df(t)}{N},\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "\\mathrm{IDF}(t) \\approx -\\log \\hat{p}(t)\n",
    "$$\n",
    "\n",
    "This is the **self-information** or **surprisal** of observing term $t$ at the corpus level.\n",
    "\n",
    "- Common terms (high $\\hat{p}(t)$) get low IDF.\n",
    "- Rare terms (low $\\hat{p}(t)$) get high IDF.\n",
    "\n",
    "Importantly, **IDF depends only on the term** $t$, not on any specific product document $d$.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. TF‚ÄìIDF: Local Probability √ó Global Surprise\n",
    "\n",
    "The TF‚ÄìIDF weight for term $t$ in product document $d$ is:\n",
    "\n",
    "$$\n",
    "w_{t,d} = \\mathrm{TF\\text{-}IDF}(t,d) = \\mathrm{TF}(t,d)\\,\\mathrm{IDF}(t).\n",
    "$$\n",
    "\n",
    "Using the information-theoretic interpretations:\n",
    "\n",
    "- $\\mathrm{TF}(t,d) \\approx \\hat{p}(t \\mid d)$ (local term probability in product document $d$),\n",
    "- $\\mathrm{IDF}(t) \\approx -\\log \\hat{p}(t)$ (global surprisal of $t$),\n",
    "\n",
    "we get:\n",
    "\n",
    "$$\n",
    "w_{t,d}\n",
    "\\;\\approx\\;\n",
    "\\hat{p}(t \\mid d)\\,\\big[-\\log \\hat{p}(t)\\big].\n",
    "$$\n",
    "\n",
    "So, for a fixed product document $d$, its TF‚ÄìIDF vector is:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^{(d)} =\n",
    "\\Big(\n",
    "\\hat{p}(t_1 \\mid d)\\,[-\\log \\hat{p}(t_1)],\n",
    "\\;\n",
    "\\hat{p}(t_2 \\mid d)\\,[-\\log \\hat{p}(t_2)],\n",
    "\\;\n",
    "\\dots,\n",
    "\\;\n",
    "\\hat{p}(t_{|V|} \\mid d)\\,[-\\log \\hat{p}(t_{|V|})]\n",
    "\\Big).\n",
    "$$\n",
    "\n",
    "This explains why **each vector component depends on the product document $d$**:\n",
    "\n",
    "- The **local factor** $\\hat{p}(t \\mid d)$ changes from product document to product document.\n",
    "- The **global factor** $-\\log \\hat{p}(t)$ is shared across the corpus.\n",
    "\n",
    "Information-theoretic intuition:\n",
    "\n",
    "- $-\\log \\hat{p}(t)$ is the information content of seeing term $t$ in general.\n",
    "- $\\hat{p}(t \\mid d)$ scales that information by how characteristic $t$ is of product document $d$.\n",
    "\n",
    "---\n",
    "### 5. Vocabulary and Query Representation in Practice\n",
    "\n",
    "In practice, the TF‚ÄìIDF **vocabulary** is built **only from product documents**, not from user queries:\n",
    "\n",
    "- We collect all product documents (titles, descriptions, etc.).\n",
    "- We apply tokenization and text preprocessing.\n",
    "- We **fit** the TF‚ÄìIDF vectorizer on these product documents:\n",
    "  - this fixes the vocabulary $V = \\{t_1, \\dots, t_{|V|}\\}$,\n",
    "  - computes $df(t)$ for each term,\n",
    "  - and derives the corresponding $\\mathrm{IDF}(t)$.\n",
    "\n",
    "A user **query** $q$ is then treated as a short text and projected into this **same vocabulary space**:\n",
    "\n",
    "- We apply the same preprocessing to $q$.\n",
    "- We compute TF for query terms (often with a simpler normalization).\n",
    "- We reuse the existing $\\mathrm{IDF}(t)$ values learned from product documents.\n",
    "\n",
    "Analogously to the product-document case, we can write:\n",
    "\n",
    "- Query term frequency (local, in the query):\n",
    "  $$\n",
    "  \\mathrm{TF}(t, q) = \\frac{f_{t,q}}{\\sum_j f_{j,q}}\n",
    "  \\;\\approx\\;\n",
    "  \\hat{p}(t \\mid q)\n",
    "  $$\n",
    "- Global IDF (reused from product documents):\n",
    "  $$\n",
    "  \\mathrm{IDF}(t) \\approx -\\log \\hat{p}(t)\n",
    "  $$\n",
    "\n",
    "So each component of the **query vector** is:\n",
    "\n",
    "$$\n",
    "w_{t,q} = \\mathrm{TF\\text{-}IDF}(t,q)\n",
    "= \\mathrm{TF}(t,q)\\,\\mathrm{IDF}(t)\n",
    "\\;\\approx\\;\n",
    "\\hat{p}(t \\mid q)\\,\\big[-\\log \\hat{p}(t)\\big].\n",
    "$$\n",
    "\n",
    "Explicitly, the query vector in the shared TF‚ÄìIDF space is:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^{(q)} =\n",
    "\\Big(\n",
    "\\hat{p}(t_1 \\mid q)\\,[-\\log \\hat{p}(t_1)],\n",
    "\\;\n",
    "\\hat{p}(t_2 \\mid q)\\,[-\\log \\hat{p}(t_2)],\n",
    "\\;\n",
    "\\dots,\n",
    "\\;\n",
    "\\hat{p}(t_{|V|} \\mid q)\\,[-\\log \\hat{p}(t_{|V|})]\n",
    "\\Big)\n",
    "\\in \\mathbb{R}^{|V|}.\n",
    "$$\n",
    "\n",
    "Important practical detail:\n",
    "\n",
    "- If a query term $t_k$ **exists** in the product vocabulary $V$, it receives a TF‚ÄìIDF weight $w_{t_k,q}$ as above.\n",
    "- If $t_k$ **never appears** in any product document, it is **out-of-vocabulary (OOV)** and is simply ignored (no coordinate in the vector).\n",
    "\n",
    "This ensures that:\n",
    "\n",
    "- Both product documents and queries live in the **same TF‚ÄìIDF vector space**, defined by the product catalog.\n",
    "- All global statistics (like $\\hat{p}(t)$ and $\\mathrm{IDF}(t)$) are meaningful with respect to the product documents we are actually ranking.\n",
    "\n",
    "---\n",
    "### 6. Product-Document Matrix View (Explicit Rectangular Form)\n",
    "\n",
    "For each product document $d_i$ and term $t_j \\in V$, the TF‚ÄìIDF weight can be written (using the information-theoretic approximations) as:\n",
    "\n",
    "$$\n",
    "\\mathrm{TF\\text{-}IDF}(t_j, d_i)\n",
    "\\;\\approx\\;\n",
    "\\hat{p}(t_j \\mid d_i)\\,\\big[-\\log \\hat{p}(t_j)\\big],\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\hat{p}(t_j \\mid d_i)$ is the empirical **local probability** of term $t_j$ in product document $d_i$ (from TF),\n",
    "- $-\\log \\hat{p}(t_j)$ is the **global surprisal** of term $t_j$ in the corpus (from IDF).\n",
    "\n",
    "If we stack all $M$ product documents, the **product-document TF‚ÄìIDF matrix** can be written explicitly as:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}\n",
    "\\;\\approx\\;\n",
    "\\begin{bmatrix}\n",
    "\\hat{p}(t_1 \\mid d_1)\\,[-\\log \\hat{p}(t_1)] & \\hat{p}(t_2 \\mid d_1)\\,[-\\log \\hat{p}(t_2)] & \\dots & \\hat{p}(t_{|V|} \\mid d_1)\\,[-\\log \\hat{p}(t_{|V|})] \\\\\n",
    "\\hat{p}(t_1 \\mid d_2)\\,[-\\log \\hat{p}(t_1)] & \\hat{p}(t_2 \\mid d_2)\\,[-\\log \\hat{p}(t_2)] & \\dots & \\hat{p}(t_{|V|} \\mid d_2)\\,[-\\log \\hat{p}(t_{|V|})] \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\hat{p}(t_1 \\mid d_M)\\,[-\\log \\hat{p}(t_1)] & \\hat{p}(t_2 \\mid d_M)\\,[-\\log \\hat{p}(t_2)] & \\dots & \\hat{p}(t_{|V|} \\mid d_M)\\,[-\\log \\hat{p}(t_{|V|})]\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{M \\times |V|}.\n",
    "$$\n",
    "\n",
    "- Row $i$ is the explicit TF‚ÄìIDF vector for product document $d_i$:\n",
    "  $$\n",
    "  \\mathbf{v}^{(d_i)} =\n",
    "  \\Big(\n",
    "  \\hat{p}(t_1 \\mid d_i)\\,[-\\log \\hat{p}(t_1)],\n",
    "  \\dots,\n",
    "  \\hat{p}(t_{|V|} \\mid d_i)\\,[-\\log \\hat{p}(t_{|V|})]\n",
    "  \\Big).\n",
    "  $$\n",
    "- Column $j$ corresponds to term $t_j$ and shows how its information-weighted presence varies across product documents.\n",
    "\n",
    "The query vector $\\mathbf{v}^{(q)}$ from Section 5 lies in the **same** $|V|$-dimensional space, so retrieval becomes a matter of comparing $\\mathbf{v}^{(q)}$ with the rows of $\\mathbf{X}$ (e.g., via cosine similarity) to rank product documents by how similar their **information profile** is to the query.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Summing the entire matrix $X$: $H(T)$ emerges (and why that matters)\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{M \\times |V|}$ with entries\n",
    "$$\n",
    "X_{i,j} = \\hat{p}(t_j \\mid d_i)\\,[-\\log \\hat{p}(t_j)].\n",
    "$$\n",
    "\n",
    "Without assuming that documents are equiprobable, fix any prior $\\pi(d_i) \\ge 0$ with $\\sum_{i=1}^{M} \\pi(d_i) = 1$, and define the **global term profile** as the mixture\n",
    "$$\n",
    "\\hat{p}(t_j) = \\sum_{i=1}^{M} \\pi(d_i)\\,\\hat{p}(t_j \\mid d_i).\n",
    "$$\n",
    "\n",
    "Then the **prior-weighted average** of all entries of $X$ is\n",
    "$$\n",
    "\\sum_{i=1}^{M} \\pi(d_i) \\sum_{j=1}^{|V|} X_{i,j}\n",
    "= \\sum_{j=1}^{|V|} \\left( \\sum_{i=1}^{M} \\pi(d_i)\\,\\hat{p}(t_j \\mid d_i) \\right)[-\\log \\hat{p}(t_j)]\n",
    "= \\sum_{j=1}^{|V|} \\hat{p}(t_j)\\,[-\\log \\hat{p}(t_j)]\n",
    "= H(T).\n",
    "$$\n",
    "\n",
    "Here, $T$ is the **global term variable** obtained by:\n",
    "\n",
    "1. picking a document $d_i$ according to $\\pi$, and  \n",
    "2. picking a token uniformly at random within that document.\n",
    "\n",
    "Under this sampling scheme, the entropy of $T$ is\n",
    "$$\n",
    "H(T) = -\\sum_{j} \\hat{p}(t_j)\\,\\log \\hat{p}(t_j),\n",
    "$$\n",
    "which we can interpret as the **catalog entropy**: the average uncertainty about which word you see when you look at a random token in the product catalog.\n",
    "\n",
    "A useful special case is when $\\pi$ is **uniform** over documents. Then $\\pi(d_i) = 1/M$, and\n",
    "$$\n",
    "\\frac{1}{M} \\sum_{i,j} X_{i,j} = H(T),\n",
    "$$\n",
    "that is,\n",
    "$$\n",
    "\\sum_{i,j} X_{i,j} = M \\cdot H(T).\n",
    "$$\n",
    "\n",
    "This immediately shows that TF‚ÄìIDF already encodes information theory:\n",
    "\n",
    "- Each entry is *local probability √ó global surprisal*:  \n",
    "  $\\hat{p}(t_j \\mid d_i)$ captures how characteristic the term $t_j$ is within product document $d_i$, while $-\\log \\hat{p}(t_j)$ measures how informative that term is across the whole catalog.\n",
    "- Aggregating entries yields canonical information-theoretic quantities:\n",
    "  - the prior-weighted global average gives the entropy $H(T)$;\n",
    "  - for a fixed document $d_i$, the sum\n",
    "    $$\n",
    "    \\sum_{j} X_{i,j}\n",
    "    $$\n",
    "    is a **cross-entropy** against the global background distribution:\n",
    "    $$\n",
    "    \\sum_{j} X_{i,j}\n",
    "    = H\\big(\\hat{p}(\\cdot \\mid d_i)\\big)\n",
    "      + D_{\\mathrm{KL}}\\big(\\hat{p}(\\cdot \\mid d_i) \\,\\|\\, \\hat{p}(\\cdot)\\big).\n",
    "    $$\n",
    "\n",
    "In short, TF‚ÄìIDF balances **local distinctiveness** with **global informativeness**‚Äîno extra assumptions required.\n",
    "\n",
    "#### How we will use this perspective going forward\n",
    "\n",
    "1. **Product representations.**  \n",
    "   We will use information-theoretic intuition to reason about how we embed product documents into a latent space (TF‚ÄìIDF, learned embeddings, or hybrids). Dimension, scaling, and normalization become decisions about how to distribute information across coordinates.\n",
    "\n",
    "2. **Query handling.**  \n",
    "   Once a good product space is set, we will:\n",
    "   - analyze how user queries are projected into the same space,\n",
    "   - measure how much information queries carry relative to the catalog,\n",
    "   - and adjust query-side processing and scoring to improve retrieval behavior.\n",
    "\n",
    "In short, whether we keep TF‚ÄìIDF, move to semantic embeddings, or add LLM-based retrieval, we will start from this information-theoretic angle to:\n",
    "\n",
    "- structure the vector spaces,\n",
    "- reason about what each dimension ‚Äúmeans‚Äù in terms of information,\n",
    "- and design improvements on both product representations and query handling to upgrade the baseline search engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec272cdd",
   "metadata": {},
   "source": [
    "# Data analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31f777",
   "metadata": {},
   "source": [
    "First, we need to analyze the data, particularly the product data, because the actual queries might differ from the query dataset, but the potential products and their characteristics are static in this case. This allows us to extract descriptive information about the necessary latent space, such as its minimum dimensions, using an information theory approach.\n",
    "\n",
    "This is why, if we choose to represent products and queries in the same n-dimensional space, as is the case with TF-IDF, where the dimensional length is the number of words in the vocabulary, the most reasonable approach is to define this space based on the product data and, once defined, proceed to adjust the rest. In this way, we will analyze how much we can compress the product information, which features we will use, and how we will represent them to avoid distortion. Once we know the optimal representation for the products, we can define this as our latent space and implement the necessary changes to the queries to embed them in that space and perform our comparison. The idea will be to find a minimal dimension space, properly designed to maintain the semantic properties of the texts once embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab57b686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_class</th>\n",
       "      <th>category hierarchy</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_features</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>solid wood platform bed</td>\n",
       "      <td>Beds</td>\n",
       "      <td>Furniture / Bedroom Furniture / Beds &amp; Headboa...</td>\n",
       "      <td>good , deep sleep can be quite difficult to ha...</td>\n",
       "      <td>overallwidth-sidetoside:64.7|dsprimaryproducts...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>all-clad 7 qt . slow cooker</td>\n",
       "      <td>Slow Cookers</td>\n",
       "      <td>Kitchen &amp; Tabletop / Small Kitchen Appliances ...</td>\n",
       "      <td>create delicious slow-cooked meals , from tend...</td>\n",
       "      <td>capacityquarts:7|producttype : slow cooker|pro...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>all-clad electrics 6.5 qt . slow cooker</td>\n",
       "      <td>Slow Cookers</td>\n",
       "      <td>Kitchen &amp; Tabletop / Small Kitchen Appliances ...</td>\n",
       "      <td>prepare home-cooked meals on any schedule with...</td>\n",
       "      <td>features : keep warm setting|capacityquarts:6....</td>\n",
       "      <td>208.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>all-clad all professional tools pizza cutter</td>\n",
       "      <td>Slicers, Peelers And Graters</td>\n",
       "      <td>Browse By Brand / All-Clad</td>\n",
       "      <td>this original stainless tool was designed to c...</td>\n",
       "      <td>overallwidth-sidetoside:3.5|warrantylength : l...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>baldwin prestige alcott passage knob with roun...</td>\n",
       "      <td>Door Knobs</td>\n",
       "      <td>Home Improvement / Doors &amp; Door Hardware / Doo...</td>\n",
       "      <td>the hardware has a rich heritage of delivering...</td>\n",
       "      <td>compatibledoorthickness:1.375 '' |countryofori...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                       product_name  \\\n",
       "0           0                            solid wood platform bed   \n",
       "1           1                        all-clad 7 qt . slow cooker   \n",
       "2           2            all-clad electrics 6.5 qt . slow cooker   \n",
       "3           3       all-clad all professional tools pizza cutter   \n",
       "4           4  baldwin prestige alcott passage knob with roun...   \n",
       "\n",
       "                  product_class  \\\n",
       "0                          Beds   \n",
       "1                  Slow Cookers   \n",
       "2                  Slow Cookers   \n",
       "3  Slicers, Peelers And Graters   \n",
       "4                    Door Knobs   \n",
       "\n",
       "                                  category hierarchy  \\\n",
       "0  Furniture / Bedroom Furniture / Beds & Headboa...   \n",
       "1  Kitchen & Tabletop / Small Kitchen Appliances ...   \n",
       "2  Kitchen & Tabletop / Small Kitchen Appliances ...   \n",
       "3                         Browse By Brand / All-Clad   \n",
       "4  Home Improvement / Doors & Door Hardware / Doo...   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  good , deep sleep can be quite difficult to ha...   \n",
       "1  create delicious slow-cooked meals , from tend...   \n",
       "2  prepare home-cooked meals on any schedule with...   \n",
       "3  this original stainless tool was designed to c...   \n",
       "4  the hardware has a rich heritage of delivering...   \n",
       "\n",
       "                                    product_features  rating_count  \\\n",
       "0  overallwidth-sidetoside:64.7|dsprimaryproducts...          15.0   \n",
       "1  capacityquarts:7|producttype : slow cooker|pro...         100.0   \n",
       "2  features : keep warm setting|capacityquarts:6....         208.0   \n",
       "3  overallwidth-sidetoside:3.5|warrantylength : l...          69.0   \n",
       "4  compatibledoorthickness:1.375 '' |countryofori...          70.0   \n",
       "\n",
       "   average_rating  review_count  \n",
       "0             4.5          15.0  \n",
       "1             2.0          98.0  \n",
       "2             3.0         181.0  \n",
       "3             4.5          42.0  \n",
       "4             5.0          42.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get products\n",
    "product_df = pd.read_csv(\"WANDS/dataset/product.csv\", sep='\\t')\n",
    "product_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ec196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d2e7b2e",
   "metadata": {},
   "source": [
    "# Continue the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42485468-77e2-4fc3-9a70-319a70603472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define functions for product search using Tf-IDF\n",
    "def calculate_tfidf(dataframe):\n",
    "    \"\"\"\n",
    "    Calculate the TF-IDF for combined product name and description.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): DataFrame with product_id, and other product information.\n",
    "\n",
    "    Returns:\n",
    "    TfidfVectorizer, csr_matrix: TF-IDF vectorizer and TF-IDF matrix.\n",
    "    \"\"\"\n",
    "    # Combine product name and description to vectorize\n",
    "    # NOTE: Please feel free to use any combination of columns available, some columns may contain NULL values\n",
    "    combined_text = dataframe['product_name'] + ' ' + dataframe['product_description']\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # convert combined_text to list of unicode strings\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_text.values.astype('U'))\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def get_top_products(vectorizer, tfidf_matrix, query, top_n=10):\n",
    "    \"\"\"\n",
    "    Get top N products for a given query based on TF-IDF similarity.\n",
    "\n",
    "    Parameters:\n",
    "    vectorizer (TfidfVectorizer): Trained TF-IDF vectorizer.\n",
    "    tfidf_matrix (csr_matrix): TF-IDF matrix for the products.\n",
    "    query (str): Search query.\n",
    "    top_n (int): Number of top products to return.\n",
    "\n",
    "    Returns:\n",
    "    list: List of top N product IDs.\n",
    "    \"\"\"\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    top_product_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    return top_product_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd06913-4149-44c7-a876-787316e1b1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define functions for evaluating retrieval performance\n",
    "def map_at_k(true_ids, predicted_ids, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Average Precision at K (MAP@K).\n",
    "\n",
    "    Parameters:\n",
    "    true_ids (list): List of relevant product IDs.\n",
    "    predicted_ids (list): List of predicted product IDs.\n",
    "    k (int): Number of top elements to consider.\n",
    "             NOTE: IF you wish to change top k, please provide a justification for choosing the new value\n",
    "\n",
    "    Returns:\n",
    "    float: MAP@K score.\n",
    "    \"\"\"\n",
    "    #if either list is empty, return 0\n",
    "    if not len(true_ids) or not len(predicted_ids):\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p_id in enumerate(predicted_ids[:k]):\n",
    "        if p_id in true_ids and p_id not in predicted_ids[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    return score / min(len(true_ids), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc51ce2-521c-428c-8475-57fbc7145d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please add any new evaluation functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47da390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get search queries\n",
    "query_df = pd.read_csv(\"WANDS/dataset/query.csv\", sep='\\t')\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d75f84-1152-43c7-b2a0-422267ab2298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get manually labeled groundtruth lables\n",
    "label_df = pd.read_csv(\"WANDS/dataset/label.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07b8f157-2049-4cdb-afc4-62e546d59dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25434</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12088</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>42931</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2636</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>42923</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  query_id  product_id       label\n",
       "0   0         0       25434       Exact\n",
       "1   1         0       12088  Irrelevant\n",
       "2   2         0       42931       Exact\n",
       "3   3         0        2636       Exact\n",
       "4   4         0       42923       Exact"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59768978-5c40-45b0-84a7-d6cc5d469b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#group the labels for each query to use when identifying exact matches\n",
    "grouped_label_df = label_df.groupby('query_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1beab6d-1f59-427b-ad9f-1f0c6fcaadcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate TF-IDF\n",
    "vectorizer, tfidf_matrix = calculate_tfidf(product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e4b8333-b747-4c3e-87bc-4825f934ab6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top products for 'armchair':\n",
      "12756 24.41 '' wide tufted polyester armchair\n",
      "42698 donham armchair\n",
      "42697 donham 25 '' wide armchair\n",
      "41270 almaraz 33.7 '' wide leather match armchair\n",
      "23907 faizah 27.6 '' wide tufted polyester armchair\n",
      "31564 biloxi 34.75 '' wide armchair\n",
      "41306 hartsell 33 '' wide armchair\n",
      "1527 howington 39 '' wide tufted linen armchair\n",
      "42802 donham polyester lounge chair\n",
      "6532 ogan 29 '' wide polyester armchair\n"
     ]
    }
   ],
   "source": [
    "#Sanity check code block to see if the search results are relevant\n",
    "#implementing a function to retrieve top K product IDs for a query\n",
    "def get_top_product_ids_for_query(query):\n",
    "    top_product_indices = get_top_products(vectorizer, tfidf_matrix, query, top_n=10)\n",
    "    top_product_ids = product_df.iloc[top_product_indices]['product_id'].tolist()\n",
    "    return top_product_ids\n",
    "\n",
    "#define the test query\n",
    "query = \"armchair\"\n",
    "\n",
    "#obtain top product IDs\n",
    "top_product_ids = get_top_product_ids_for_query(query)\n",
    "\n",
    "print(f\"Top products for '{query}':\")\n",
    "for product_id in top_product_ids:\n",
    "    product = product_df.loc[product_df['product_id'] == product_id]\n",
    "    print(product_id, product['product_name'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bdeda61-7fb0-4ca6-a9e1-f1789b539f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#implementing a function to retrieve exact match product IDs for a query_id\n",
    "def get_exact_matches_for_query(query_id):\n",
    "    query_group = grouped_label_df.get_group(query_id)\n",
    "    exact_matches = query_group.loc[query_group['label'] == 'Exact']['product_id'].values\n",
    "    return exact_matches\n",
    "\n",
    "#applying the function to obtain top product IDs and adding top K product IDs to the dataframe \n",
    "query_df['top_product_ids'] = query_df['query'].apply(get_top_product_ids_for_query)\n",
    "\n",
    "#adding the list of exact match product_IDs from labels_df\n",
    "query_df['relevant_ids'] = query_df['query_id'].apply(get_exact_matches_for_query)\n",
    "\n",
    "#now assign the map@k score\n",
    "query_df['map@k'] = query_df.apply(lambda x: map_at_k(x['relevant_ids'], x['top_product_ids'], k=10), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed01f293-d87b-4ab0-811a-d39140ed5638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29319550540123457"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the MAP across the entire query set\n",
    "query_df.loc[:, 'map@k'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardvard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
